{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset de revisiones físicas\n",
    "\n",
    "### Limpieza inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "df_check = pd.read_csv('../Dataset/datos_check.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negocio_id</th>\n",
       "      <th>fecha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--1UhMGODdWsrMastO9DZw</td>\n",
       "      <td>2016-04-26 19:49:16, 2016-08-30 18:36:57, 2016...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--6MefnULPED_I942VcFNA</td>\n",
       "      <td>2011-06-04 18:22:23, 2011-07-23 23:51:33, 2012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--7zmmkVg-IMGaXbuVd0SQ</td>\n",
       "      <td>2014-12-29 19:25:50, 2015-01-17 01:49:14, 2015...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>--8LPVSo5i0Oo61X01sV9A</td>\n",
       "      <td>2016-07-08 16:43:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>--9QQLMTbFzLJ_oT-ON3Xw</td>\n",
       "      <td>2010-06-26 17:39:07, 2010-08-01 20:06:21, 2010...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161945</th>\n",
       "      <td>zzvlwkcNR1CCqOPXwuvz2A</td>\n",
       "      <td>2017-05-06 20:05:15, 2017-05-12 22:37:03, 2017...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161946</th>\n",
       "      <td>zzwaS0xn1MVEPEf0hNLjew</td>\n",
       "      <td>2010-02-16 02:09:56, 2010-07-05 05:40:48, 2010...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161947</th>\n",
       "      <td>zzwhN7x37nyjP0ZM8oiHmw</td>\n",
       "      <td>2016-03-06 13:27:02, 2016-03-09 00:41:53, 2016...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161948</th>\n",
       "      <td>zzwicjPC9g246MK2M1ZFBA</td>\n",
       "      <td>2012-09-22 00:26:15, 2012-09-23 20:12:00, 2012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161949</th>\n",
       "      <td>zzzaIBwimxVej4tY6qFOUQ</td>\n",
       "      <td>2012-10-12 16:26:35, 2012-10-19 19:31:14, 2012...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>161950 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    negocio_id  \\\n",
       "0       --1UhMGODdWsrMastO9DZw   \n",
       "1       --6MefnULPED_I942VcFNA   \n",
       "2       --7zmmkVg-IMGaXbuVd0SQ   \n",
       "3       --8LPVSo5i0Oo61X01sV9A   \n",
       "4       --9QQLMTbFzLJ_oT-ON3Xw   \n",
       "...                        ...   \n",
       "161945  zzvlwkcNR1CCqOPXwuvz2A   \n",
       "161946  zzwaS0xn1MVEPEf0hNLjew   \n",
       "161947  zzwhN7x37nyjP0ZM8oiHmw   \n",
       "161948  zzwicjPC9g246MK2M1ZFBA   \n",
       "161949  zzzaIBwimxVej4tY6qFOUQ   \n",
       "\n",
       "                                                    fecha  \n",
       "0       2016-04-26 19:49:16, 2016-08-30 18:36:57, 2016...  \n",
       "1       2011-06-04 18:22:23, 2011-07-23 23:51:33, 2012...  \n",
       "2       2014-12-29 19:25:50, 2015-01-17 01:49:14, 2015...  \n",
       "3                                     2016-07-08 16:43:30  \n",
       "4       2010-06-26 17:39:07, 2010-08-01 20:06:21, 2010...  \n",
       "...                                                   ...  \n",
       "161945  2017-05-06 20:05:15, 2017-05-12 22:37:03, 2017...  \n",
       "161946  2010-02-16 02:09:56, 2010-07-05 05:40:48, 2010...  \n",
       "161947  2016-03-06 13:27:02, 2016-03-09 00:41:53, 2016...  \n",
       "161948  2012-09-22 00:26:15, 2012-09-23 20:12:00, 2012...  \n",
       "161949  2012-10-12 16:26:35, 2012-10-19 19:31:14, 2012...  \n",
       "\n",
       "[161950 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Búsqueda de valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Hay valores nulos?\n",
      " negocio_id    False\n",
      "fecha         False\n",
      "dtype: bool \n",
      "\n",
      "¿Cuantos?\n",
      " negocio_id    0\n",
      "fecha         0\n",
      "dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"¿Hay valores nulos?\\n\", pd.isnull(df_check).any(), \"\\n\")\n",
    "print(\"¿Cuantos?\\n\", pd.isnull(df_check).sum(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprobación sintáctica de la fecha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se imprime ningún error por lo que, aparentemente, todas las fechas son sintácticamente correctas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         False\n",
       "1         False\n",
       "2         False\n",
       "3         False\n",
       "4         False\n",
       "          ...  \n",
       "161945    False\n",
       "161946    False\n",
       "161947    False\n",
       "161948    False\n",
       "161949    False\n",
       "Name: fecha, Length: 161950, dtype: bool"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "pattern = re.compile(\"[\\s]?[0-9][0-9][0-9][0-9][-][0-1][0-9][-][0-3][0-9]\\s[0-2][0-9][:][0-5][0-9][:][0-5][0-9]\")\n",
    "\n",
    "\n",
    "def preprocesa(x):\n",
    "    listaFechas = x.split(\",\")\n",
    "    for date in listaFechas:\n",
    "        if(pattern.match(date) == None):\n",
    "            print(\"date not matching: \" + str(x))\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "df_check['fecha'].apply(lambda x: preprocesa(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANTE \n",
    "A continuacion vamos a tratar de sacar informacion del los datos. Debido a que solo hay dos columnas una identificadora del negocio y otra que contiene una listas de fechas con las revisiones de cada negocio, la mayoria de informacion se encuentra en esta columna y es la que vamos a tratar. Se tendran dos enfoques:\n",
    "\n",
    "-Generar un dataframe con el numero de revisiones que tiene cada negocio y el intervalo de las revisiones. Centrandonos mas en un solo negocio.\n",
    "\n",
    "-Generar un dataframe separando las listas de los negocios de tuplas negocio/revision. Y posteriormente separar esa fecha de negocio en año mes dia hora minuto y dia de la semana para tratar de generar graficos agrupando por esos atributos y ver si de manera global las revisiones depende de la fecha en la que se hacen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos el numero de revisiones que tiene cada negocio y creamos una columna nueva para cada negocio. Con esto podremos determinar que negocios son los que mas revisiones tienen y si llegan a tener demasiadas investigar a que se debe este fenomeno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_revisiones(x):\n",
    "    listaFechas = x.split(\",\")\n",
    "    return len(listaFechas)\n",
    "df_check['num_revisiones']=df_check['fecha'].apply(lambda x:n_revisiones(x))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertimos el formato string de la columna fecha en datetime para poder operar con el posteriormente con ellas y manejarlas con facilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pasarfechas(x):\n",
    "    fechas=[]\n",
    "    listaFechas = x.split(\",\")\n",
    "    for i in listaFechas:\n",
    "        date=datetime.datetime.strptime(i.strip(),'%Y-%m-%d %H:%M:%S')\n",
    "        fechas.append(date)\n",
    "    return fechas\n",
    "df_check['fechas']=df_check['fecha'].apply(lambda x:pasarfechas(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mediarevisiones=df_check\n",
    "def media(x):\n",
    "    colum=0\n",
    "    aux=[None]*(len(x)-1)\n",
    "    if(len(x)==1):\n",
    "        return 0\n",
    "    while colum < (len(x)-1):\n",
    "        resultado=(x[colum]-x[colum+1])\n",
    "        aux[colum]=resultado\n",
    "        colum=colum+1\n",
    "    i=0\n",
    "    suma=aux[0]-aux[0]\n",
    "    while i < (len(x)-1):\n",
    "        suma=suma+aux[i]\n",
    "        i=i+1\n",
    "    media=-suma/(len(x)-1)\n",
    "    del aux\n",
    "    return media\n",
    "df_mediarevisiones['media']=df_mediarevisiones['fechas'].apply(lambda x:media(x))\n",
    "df_mediarevisiones=df_mediarevisiones.drop(['fecha'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mediarevisiones.to_pickle(\"../Dataset/df_checksmedias_revisiones.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sabiendo que la gran cantidad de revisiones que puede llegar a tener un solo negocio se han tomado medidas para poder sacar cada tupla revision/negocio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_revisiones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>161950.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>117.870627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>733.721561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>65.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>143061.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       num_revisiones\n",
       "count   161950.000000\n",
       "mean       117.870627\n",
       "std        733.721561\n",
       "min          1.000000\n",
       "25%          5.000000\n",
       "50%         16.000000\n",
       "75%         65.000000\n",
       "max     143061.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_check.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importante\n",
    "Acontinuacion se quieren separar las listas de revisiones de cada negocio en tuplas para generar el segundo dataframe.\n",
    "Debido a que no se puede tratar entero dataframe check este se tiene que partir en distintos dataframes que separen las revisiones y luego juntarlas de nuevo en otro fichero. Los dataframes en los que se divide son:\n",
    "\n",
    "-**Df_Checking_menor**: Con rangos de numero de revisiones de 0 a 69.\n",
    "\n",
    "-**df_checking_medio**: Con rangos de numero de revisiones de 70 a 499\n",
    "\n",
    "-**df_checking_alto_medio**: Con rangos de numero de revisiones de 500 a 2999.\n",
    "\n",
    "-**df_checking_alto_alto**: Con rangos de numero de revisiones de 3000 a 999999\n",
    "\n",
    "-**df_checking_demasiado**: Con rangos de numero de revisiones de 100000 al maximo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_checking=df_check.drop(df_check[df_check.num_revisiones>70].index)\n",
    "df_checking=df_checking.drop(['num_revisiones','fecha','media'],axis=1)\n",
    "fecha=df_checking['fechas'].apply(pd.Series).reset_index().melt(id_vars='index').dropna()[['index','value']].set_index('index')\n",
    "df_checking=pd.merge(fecha,df_checking[['negocio_id']],left_index=True,right_index=True).rename(columns={'value_x': 'negocio_id', 'value_y' :'fechas'})\n",
    "df_checking.to_pickle(\"../Dataset/dfcheckmenor.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_checking=df_check.drop(df_check[df_check.num_revisiones>500].index)\n",
    "df_checking=df_checking.drop(df_checking[df_checking.num_revisiones<70].index)\n",
    "df_checking=df_checking.drop(['num_revisiones','fecha','media','media'],axis=1)\n",
    "fecha=df_checking['fechas'].apply(pd.Series).reset_index().melt(id_vars='index').dropna()[['index','value']].set_index('index')\n",
    "df_checking=pd.merge(fecha,df_checking[['negocio_id']],left_index=True,right_index=True).rename(columns={'value_x': 'negocio_id', 'value_y' :'fechas'})\n",
    "df_checking.to_pickle(\"../Dataset/dfcheckmedio.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_checking=df_check.drop(df_check[df_check.num_revisiones>3000].index)\n",
    "df_checking=df_checking.drop(df_checking[df_checking.num_revisiones<500].index)\n",
    "df_checking=df_checking.drop(['num_revisiones','fecha','media'],axis=1)\n",
    "fecha=df_checking['fechas'].apply(pd.Series).reset_index().melt(id_vars='index').dropna()[['index','value']].set_index('index')\n",
    "df_checking=pd.merge(fecha,df_checking[['negocio_id']],left_index=True,right_index=True).rename(columns={'value_x': 'negocio_id', 'value_y' :'fechas'})\n",
    "df_checking.to_pickle(\"../Dataset/dfcheckaltomedio.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_checking=df_check.drop(df_check[df_check.num_revisiones>100000].index)\n",
    "df_checking=df_checking.drop(df_checking[df_checking.num_revisiones<3000].index)\n",
    "df_checking=df_checking.drop(['num_revisiones','fecha','media'],axis=1)\n",
    "fecha=df_checking['fechas'].apply(pd.Series).reset_index().melt(id_vars='index').dropna()[['index','value']].set_index('index')\n",
    "df_checking=pd.merge(fecha,df_checking[['negocio_id']],left_index=True,right_index=True).rename(columns={'value_x': 'negocio_id', 'value_y' :'fechas'})\n",
    "df_checking.to_pickle(\"../Dataset/dfcheckaltoalto.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_checking=df_check.drop(df_check[df_check.num_revisiones<100000].index)\n",
    "df_checking=df_checking.drop(['num_revisiones','fecha','media'],axis=1)\n",
    "fecha=df_checking['fechas'].apply(pd.Series).reset_index().melt(id_vars='index').dropna()[['index','value']].set_index('index')\n",
    "df_checking=pd.merge(fecha,df_checking[['negocio_id']],left_index=True,right_index=True).rename(columns={'value_x': 'negocio_id', 'value_y' :'fechas'})\n",
    "df_checking.to_pickle(\"../Dataset/dfcheckdemasiado.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracion de los ficheros PKL \n",
    "\n",
    "Posteriormente cada uno de los dataframes calculados se guarda en un fichero pkl para su posterior union.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcheckmenor= pd.read_pickle(\"../Dataset/dfcheckmenor.pkl\")\n",
    "dfcheckmedio= pd.read_pickle(\"../Dataset/dfcheckmedio.pkl\")\n",
    "dfcheckaltomedio= pd.read_pickle(\"../Dataset/dfcheckaltomedio.pkl\")\n",
    "dfcheckaltoalto= pd.read_pickle(\"../Dataset/dfcheckaltoalto.pkl\")\n",
    "dfcheckdemasiado= pd.read_pickle(\"../Dataset/dfcheckdemasiado.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=dfcheckmenor.append(dfcheckmedio)\n",
    "b=a.append(dfcheckaltomedio)\n",
    "a=b.append(dfcheckaltoalto)\n",
    "b=a.append(dfcheckdemasiado)\n",
    "b=b.sort_index()\n",
    "b=b.rename(columns={\"value\":'fecha'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "b['anyo'] = pd.DatetimeIndex(b['fecha']).year\n",
    "b['mes'] = pd.DatetimeIndex(b['fecha']).month\n",
    "b['dia'] = pd.DatetimeIndex(b['fecha']).day\n",
    "b['hora'] = pd.DatetimeIndex(b['fecha']).hour\n",
    "b['minuto'] = pd.DatetimeIndex(b['fecha']).minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.to_pickle(\"../Dataset/df_check_timestamp_separado.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
